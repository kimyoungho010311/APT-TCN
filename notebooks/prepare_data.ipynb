{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8081bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.12.0\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# PART 0: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "# ===================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df7ddee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===================================================================\n",
    "# PART 1: ê²½ë¡œ ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "# ===================================================================\n",
    "# --- ì…ë ¥ ë°ì´í„° ê²½ë¡œ ---\n",
    "IMAGE_DIRECTORY = '../data/interim/satellites'\n",
    "TABULAR_DATA_PATH = '../data/interim/gang_nam_sendimental_score_with_sale.csv'\n",
    "\n",
    "# --- ì¶œë ¥ ë°ì´í„° ë° ëª¨ë¸ ì €ì¥ ê²½ë¡œ ---\n",
    "# ì €ì¥ ê²½ë¡œê°€ ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ìƒì„±\n",
    "os.makedirs('../data/final', exist_ok=True)\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# ê° ë°ì´í„°ì…‹(train, val, test)ë³„ë¡œ ë²¡í„° íŒŒì¼ì„ ì €ì¥í•  ê²½ë¡œ í…œí”Œë¦¿\n",
    "IMAGE_VECTOR_SAVE_PATH = '../data/final/image_features_{split}_32d.npy'\n",
    "TABULAR_VECTOR_SAVE_PATH = '../data/final/tabular_features_{split}_64d.npy'\n",
    "\n",
    "# íŠ¹ì§• ì¶”ì¶œì— ì‚¬ìš©ëœ ëª¨ë¸(PCA, Scaler) ì €ì¥ ê²½ë¡œ\n",
    "PCA_MODEL_SAVE_PATH = '../models/pca_image_feature_extractor.pkl'\n",
    "SCALER_SAVE_PATH = '../models/tabular_minmax_scaler.pkl'\n",
    "\n",
    "# --- í•˜ì´í¼íŒŒë¼ë¯¸í„° ---\n",
    "IMAGE_PCA_COMPONENTS = 32\n",
    "TABULAR_MLP_COMPONENTS = 64\n",
    "BATCH_SIZE = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80fd3d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PART 2: ë°ì´í„° ë¡œë”© ë° ë¶„í•  ì‹œì‘...\n",
      "ë°ì´í„° ë¶„í•  ì™„ë£Œ: Train=15092, Validation=4312, Test=2157\n",
      "PART 2: ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===================================================================\n",
    "# PART 2: ë°ì´í„° ë¡œë”© ë° ë¶„í•  (Data Loading & Splitting)\n",
    "# ëª¨ë“  ì „ì²˜ë¦¬ ì´ì „ì— ë°ì´í„°ë¥¼ ì‹œê°„ìˆœìœ¼ë¡œ ë¶„í• í•˜ì—¬ ë°ì´í„° ìœ ì¶œ(Data Leakage)ì„ ë°©ì§€í•©ë‹ˆë‹¤.\n",
    "# ===================================================================\n",
    "print(\"\\nPART 2: ë°ì´í„° ë¡œë”© ë° ë¶„í•  ì‹œì‘...\")\n",
    "df = pd.read_csv(TABULAR_DATA_PATH)\n",
    "df['ê³„ì•½ì¼ì'] = pd.to_datetime(df['ê³„ì•½ì¼ì'])\n",
    "df = df.sort_values('ê³„ì•½ì¼ì').reset_index(drop=True)\n",
    "\n",
    "# ì´ë¯¸ì§€ ê²½ë¡œ ìƒì„±\n",
    "df['image_filename'] = [f'apt_image_{i}.jpg' for i in df.index]\n",
    "df['image_path'] = df['image_filename'].apply(lambda f: os.path.join(IMAGE_DIRECTORY, f))\n",
    "\n",
    "# ë°ì´í„° ë¶„í•  (7:2:1 ë¹„ìœ¨)\n",
    "n_total = len(df)\n",
    "train_end = int(n_total * 0.7)\n",
    "val_end = int(n_total * 0.9)\n",
    "\n",
    "train_df = df[:train_end].copy()\n",
    "val_df = df[train_end:val_end].copy()\n",
    "test_df = df[val_end:].copy()\n",
    "\n",
    "print(f\"ë°ì´í„° ë¶„í•  ì™„ë£Œ: Train={len(train_df)}, Validation={len(val_df)}, Test={len(test_df)}\")\n",
    "print(\"PART 2: ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31938dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PART 3: ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ ì‹œì‘...\n",
      "í›ˆë ¨ ë°ì´í„°ë¡œ PCA ëª¨ë¸ í•™ìŠµ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 16:46:10.985233: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472/472 [==============================] - 404s 854ms/step\n",
      "PCA ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨: 0.7215\n",
      "ê²€ì¦ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„° ë³€í™˜...\n",
      "135/135 [==============================] - 119s 877ms/step\n",
      "68/68 [==============================] - 52s 763ms/step\n",
      "ì´ë¯¸ì§€ íŠ¹ì§• ë²¡í„° 3ê°œì™€ PCA ëª¨ë¸ì´ '../models' ë° '../data/final'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "PART 3: ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===================================================================\n",
    "# PART 3: ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ (Image Feature Extraction)\n",
    "# ì¤‘ìš”: PCA ëª¨ë¸ì€ í›ˆë ¨ ë°ì´í„°(train_df)ì—ë§Œ `fit`í•˜ê³ , ëª¨ë“  ë°ì´í„°ì…‹ì— `transform`ì„ ì ìš©í•©ë‹ˆë‹¤.\n",
    "# ===================================================================\n",
    "print(\"\\nPART 3: ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ ì‹œì‘...\")\n",
    "\n",
    "def extract_image_features_2048d(image_paths, batch_size=BATCH_SIZE):\n",
    "    \"\"\"ResNet50ìœ¼ë¡œ ê³ ì°¨ì› íŠ¹ì§• ë²¡í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” í—¬í¼ í•¨ìˆ˜\"\"\"\n",
    "    image_input = Input(shape=(224, 224, 3), name='image_input')\n",
    "    base_cnn = ResNet50(weights='imagenet', include_top=False, pooling='avg', input_tensor=image_input)\n",
    "    base_cnn.trainable = False\n",
    "    extractor_model = Model(inputs=image_input, outputs=base_cnn.output)\n",
    "    \n",
    "    def image_generator(paths, b_size):\n",
    "        for i in range(0, len(paths), b_size):\n",
    "            batch_paths = paths[i:i+b_size]\n",
    "            batch_images = []\n",
    "            for p in batch_paths:\n",
    "                try:\n",
    "                    img = load_img(p, target_size=(224, 224))\n",
    "                    img_array = img_to_array(img)\n",
    "                    batch_images.append(img_array)\n",
    "                except (FileNotFoundError, IOError):\n",
    "                    print(f\"ê²½ê³ : {p} ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 0ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.\")\n",
    "                    batch_images.append(np.zeros((224, 224, 3)))\n",
    "            yield preprocess_input(np.array(batch_images))\n",
    "            \n",
    "    num_batches = int(np.ceil(len(image_paths) / batch_size))\n",
    "    features = extractor_model.predict(image_generator(image_paths, batch_size), steps=num_batches, verbose=1)\n",
    "    return features\n",
    "\n",
    "# 1. í›ˆë ¨ ë°ì´í„°ì˜ ì´ë¯¸ì§€ë¡œ PCA ëª¨ë¸ í•™ìŠµ(fit)\n",
    "print(\"í›ˆë ¨ ë°ì´í„°ë¡œ PCA ëª¨ë¸ í•™ìŠµ...\")\n",
    "train_image_features_2048d = extract_image_features_2048d(train_df['image_path'].tolist())\n",
    "pca = PCA(n_components=IMAGE_PCA_COMPONENTS)\n",
    "train_image_features_32d = pca.fit_transform(train_image_features_2048d)\n",
    "print(f\"PCA ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨: {np.sum(pca.explained_variance_ratio_):.4f}\")\n",
    "\n",
    "# 2. í•™ìŠµëœ PCA ëª¨ë¸ë¡œ ê²€ì¦ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„° ë³€í™˜(transform)\n",
    "print(\"ê²€ì¦ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„° ë³€í™˜...\")\n",
    "val_image_features_2048d = extract_image_features_2048d(val_df['image_path'].tolist())\n",
    "val_image_features_32d = pca.transform(val_image_features_2048d)\n",
    "\n",
    "test_image_features_2048d = extract_image_features_2048d(test_df['image_path'].tolist())\n",
    "test_image_features_32d = pca.transform(test_image_features_2048d)\n",
    "\n",
    "# 3. íŠ¹ì§• ë²¡í„° ë° PCA ëª¨ë¸ ì €ì¥\n",
    "np.save(IMAGE_VECTOR_SAVE_PATH.format(split='train'), train_image_features_32d)\n",
    "np.save(IMAGE_VECTOR_SAVE_PATH.format(split='val'), val_image_features_32d)\n",
    "np.save(IMAGE_VECTOR_SAVE_PATH.format(split='test'), test_image_features_32d)\n",
    "joblib.dump(pca, PCA_MODEL_SAVE_PATH)\n",
    "print(f\"ì´ë¯¸ì§€ íŠ¹ì§• ë²¡í„° 3ê°œì™€ PCA ëª¨ë¸ì´ '{os.path.dirname(PCA_MODEL_SAVE_PATH)}' ë° '{os.path.dirname(IMAGE_VECTOR_SAVE_PATH)}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "print(\"PART 3: ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f442bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PART 4: ì •í˜• ë°ì´í„° íŠ¹ì§• ì¶”ì¶œ ì‹œì‘...\n",
      "MLPë¥¼ ì´ìš©í•´ ì •í˜• ë°ì´í„° íŠ¹ì§• ì¶”ì¶œ ì¤‘...\n",
      "472/472 [==============================] - 0s 157us/step\n",
      "135/135 [==============================] - 0s 159us/step\n",
      "68/68 [==============================] - 0s 168us/step\n",
      "ì •í˜• ë°ì´í„° íŠ¹ì§• ë²¡í„° 3ê°œì™€ ìŠ¤ì¼€ì¼ëŸ¬ê°€ '../models' ë° '../data/final'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "PART 4: ì™„ë£Œ!\n",
      "\n",
      "ğŸ‰ ëª¨ë“  ë°ì´í„° ì¤€ë¹„ ê³¼ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===================================================================\n",
    "# PART 4: ì •í˜• ë°ì´í„° íŠ¹ì§• ì¶”ì¶œ (Tabular Feature Extraction)\n",
    "# ì¤‘ìš”: MinMaxScalerëŠ” í›ˆë ¨ ë°ì´í„°ì—ë§Œ `fit`í•˜ê³ , ëª¨ë“  ë°ì´í„°ì…‹ì— `transform`ì„ ì ìš©í•©ë‹ˆë‹¤.\n",
    "# ===================================================================\n",
    "print(\"\\nPART 4: ì •í˜• ë°ì´í„° íŠ¹ì§• ì¶”ì¶œ ì‹œì‘...\")\n",
    "\n",
    "tabular_cols = [\n",
    "    'ì „ìš©ë©´ì (ã¡)', 'ì¸µ', 'ê±´ì¶•ë…„ë„', 'ì•„íŒŒíŠ¸ ë‚˜ì´', 'leading_index', 'ê±´ì„¤ê¸°ì„±ì•¡(ë°±ë§Œì›)', \n",
    "    'ë¶€ë™ì‚°_ì†Œë¹„ì‹¬ë¦¬ì§€ìˆ˜', 'ì£¼íƒì‹œì¥_ì†Œë¹„ì‹¬ë¦¬ì§€ìˆ˜', 'ê°•ë‚¨êµ¬_ë³€ë™ë¥ ', 'ê°•ë‚¨êµ¬_ëˆ„ê³„', \n",
    "    'í† ì§€ì‹œì¥_ì†Œë¹„ì‹¬ë¦¬ì§€ìˆ˜', 'ì•„íŒŒíŠ¸_í˜¸ìˆ˜', 'ì•„íŒŒíŠ¸_ë©´ì ', 'rate'\n",
    "]\n",
    "\n",
    "# 1. í›ˆë ¨ ë°ì´í„°ë¡œ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ(fit)\n",
    "scaler = MinMaxScaler()\n",
    "train_tabular_scaled = scaler.fit_transform(train_df[tabular_cols])\n",
    "\n",
    "# 2. í•™ìŠµëœ ìŠ¤ì¼€ì¼ëŸ¬ë¡œ ê²€ì¦ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„° ë³€í™˜(transform)\n",
    "val_tabular_scaled = scaler.transform(val_df[tabular_cols])\n",
    "test_tabular_scaled = scaler.transform(test_df[tabular_cols])\n",
    "\n",
    "# 3. MLP íŠ¹ì§• ì¶”ì¶œê¸° ëª¨ë¸ ìƒì„±\n",
    "mlp_input = Input(shape=(len(tabular_cols),), name='tabular_input')\n",
    "embedding_layer = Dense(TABULAR_MLP_COMPONENTS, activation='relu')(mlp_input)\n",
    "mlp_extractor = Model(inputs=mlp_input, outputs=embedding_layer)\n",
    "\n",
    "# 4. ê° ë°ì´í„°ì…‹ì„ MLPë¡œ ë³€í™˜\n",
    "print(\"MLPë¥¼ ì´ìš©í•´ ì •í˜• ë°ì´í„° íŠ¹ì§• ì¶”ì¶œ ì¤‘...\")\n",
    "train_tabular_features_64d = mlp_extractor.predict(train_tabular_scaled)\n",
    "val_tabular_features_64d = mlp_extractor.predict(val_tabular_scaled)\n",
    "test_tabular_features_64d = mlp_extractor.predict(test_tabular_scaled)\n",
    "\n",
    "# 5. íŠ¹ì§• ë²¡í„° ë° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥\n",
    "np.save(TABULAR_VECTOR_SAVE_PATH.format(split='train'), train_tabular_features_64d)\n",
    "np.save(TABULAR_VECTOR_SAVE_PATH.format(split='val'), val_tabular_features_64d)\n",
    "np.save(TABULAR_VECTOR_SAVE_PATH.format(split='test'), test_tabular_features_64d)\n",
    "joblib.dump(scaler, SCALER_SAVE_PATH)\n",
    "print(f\"ì •í˜• ë°ì´í„° íŠ¹ì§• ë²¡í„° 3ê°œì™€ ìŠ¤ì¼€ì¼ëŸ¬ê°€ '{os.path.dirname(SCALER_SAVE_PATH)}' ë° '{os.path.dirname(TABULAR_VECTOR_SAVE_PATH)}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "print(\"PART 4: ì™„ë£Œ!\")\n",
    "\n",
    "print(\"\\nğŸ‰ ëª¨ë“  ë°ì´í„° ì¤€ë¹„ ê³¼ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca7110d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apt_sale",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
